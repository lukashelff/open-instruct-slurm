{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspect GRPO rollout traces\n",
        "\n",
        "Rollouts are saved as **tokenized** JSONL: one JSON object per line with `prompt_tokens`, `response_tokens` (token IDs), and optionally `logprobs` (vLLM logprobs per response token). Files can be very large (multi-GB). This notebook streams them line-by-line to avoid loading everything into memory.\n",
        "\n",
        "**RolloutRecord fields:** `step`, `sample_idx`, `prompt_idx`, `prompt_tokens`, `response_tokens`, `reward`, `advantage`, `finish_reason`, `dataset`, `ground_truth`, `request_info`, `logprobs`\n",
        "\n",
        "**Note:** The saved `logprobs` are from **vLLM** at rollout time. To compare with local policy logprobs (e.g. for debugging `vllm_vs_local_logprob_diff_mean`), you would need to run a separate forward pass on the same tokens with the learner model; this notebook only inspects the stored vLLM logprobs and metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the rollouts directory (contains *_metadata.jsonl and *_rollouts_*.jsonl)\n",
        "ROLLOUTS_DIR = \"/mnt/vast/home/lh22zyta/shortcut-RL/open-instruct/output/RLVR-soofi-Olmo-IsomorphicRL/rollouts\"\n",
        "\n",
        "# Optional: restrict to a specific run (prefix of rollout filenames). If None, use first run found.\n",
        "RUN_NAME = \"RLVR-soofi-Olmo-IsomorphicRL__1__1772176743\"  # or None for auto\n",
        "\n",
        "# Limit number of lines to read per file (None = no limit). Use for quick checks on huge files.\n",
        "MAX_LINES_PER_FILE = None  # e.g. 50_000\n",
        "\n",
        "# Step range to load (None = all steps). Reduces memory when inspecting a window.\n",
        "STEP_MIN = None  # e.g. 400\n",
        "STEP_MAX = None  # e.g. 600\n",
        "\n",
        "# How many full records to keep in memory for detailed inspection (rest are aggregated as stats only)\n",
        "NUM_SAMPLE_RECORDS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List rollout files and load metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_rollout_files(rollouts_dir: str, run_name: str | None = None) -> tuple[list[str], str | None, dict | None]:\n",
        "    \"\"\"List rollout JSONL paths and return (paths, run_name, metadata_dict).\"\"\"\n",
        "    base = Path(rollouts_dir)\n",
        "    if not base.is_dir():\n",
        "        raise FileNotFoundError(f\"Not a directory: {rollouts_dir}\")\n",
        "\n",
        "    if run_name is None:\n",
        "        # Infer run_name from first metadata file\n",
        "        metas = sorted(base.glob(\"*_metadata.jsonl\"))\n",
        "        if not metas:\n",
        "            raise FileNotFoundError(f\"No *_metadata.jsonl in {rollouts_dir}\")\n",
        "        run_name = metas[0].stem.replace(\"_metadata\", \"\")\n",
        "\n",
        "    meta_path = base / f\"{run_name}_metadata.jsonl\"\n",
        "    metadata = None\n",
        "    if meta_path.exists():\n",
        "        with open(meta_path) as f:\n",
        "            metadata = json.loads(f.readline())\n",
        "\n",
        "    paths = sorted(base.glob(f\"{run_name}_rollouts_*.jsonl\"))\n",
        "    return [str(p) for p in paths], run_name, metadata\n",
        "\n",
        "\n",
        "paths, run_name, metadata = get_rollout_files(ROLLOUTS_DIR, RUN_NAME)\n",
        "print(f\"Run: {run_name}\")\n",
        "print(f\"Rollout files: {len(paths)}\")\n",
        "for p in paths:\n",
        "    size_mb = os.path.getsize(p) / (1024 * 1024)\n",
        "    print(f\"  {os.path.basename(p)}  ({size_mb:.1f} MB)\")\n",
        "if metadata:\n",
        "    print(f\"Metadata: {metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stream rollouts and aggregate stats (memory-efficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_rollouts(\n",
        "    paths: list[str],\n",
        "    step_min: int | None = None,\n",
        "    step_max: int | None = None,\n",
        "    max_lines_per_file: int | None = None,\n",
        "    num_sample_records: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterate over JSONL files line-by-line. For each record:\n",
        "    - Aggregate per-step stats (reward, advantage, response length, logprobs).\n",
        "    - Keep up to num_sample_records full records (spread across steps) for inspection.\n",
        "    \"\"\"\n",
        "    step_records = defaultdict(list)  # step -> list of lightweight dicts\n",
        "    sample_records = []  # full records for display\n",
        "    steps_seen = set()\n",
        "    total_lines = 0\n",
        "\n",
        "    for filepath in paths:\n",
        "        lines_read = 0\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                if max_lines_per_file is not None and lines_read >= max_lines_per_file:\n",
        "                    break\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    rec = json.loads(line)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Skip bad line in {filepath} line {lines_read + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                step = rec.get(\"step\", -1)\n",
        "                if step_min is not None and step < step_min:\n",
        "                    lines_read += 1\n",
        "                    total_lines += 1\n",
        "                    continue\n",
        "                if step_max is not None and step > step_max:\n",
        "                    lines_read += 1\n",
        "                    total_lines += 1\n",
        "                    continue\n",
        "\n",
        "                reward = rec.get(\"reward\", 0.0)\n",
        "                advantage = rec.get(\"advantage\", 0.0)\n",
        "                resp_tokens = rec.get(\"response_tokens\", [])\n",
        "                logprobs = rec.get(\"logprobs\")\n",
        "\n",
        "                stat = {\n",
        "                    \"reward\": reward,\n",
        "                    \"advantage\": advantage,\n",
        "                    \"response_len\": len(resp_tokens),\n",
        "                    \"prompt_len\": len(rec.get(\"prompt_tokens\", [])),\n",
        "                    \"finish_reason\": rec.get(\"finish_reason\", \"\"),\n",
        "                }\n",
        "                if logprobs is not None:\n",
        "                    valid = [x for x in logprobs if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "                    if valid:\n",
        "                        stat[\"logprob_mean\"] = float(np.mean(valid))\n",
        "                        stat[\"logprob_std\"] = float(np.std(valid))\n",
        "                        stat[\"logprob_min\"] = float(np.min(valid))\n",
        "                        stat[\"logprob_max\"] = float(np.max(valid))\n",
        "                    else:\n",
        "                        stat[\"logprob_mean\"] = stat[\"logprob_std\"] = stat[\"logprob_min\"] = stat[\"logprob_max\"] = None\n",
        "                else:\n",
        "                    stat[\"logprob_mean\"] = stat[\"logprob_std\"] = stat[\"logprob_min\"] = stat[\"logprob_max\"] = None\n",
        "\n",
        "                step_records[step].append(stat)\n",
        "\n",
        "                # Keep a few full records for inspection (one per step span)\n",
        "                if len(sample_records) < num_sample_records and step not in steps_seen:\n",
        "                    steps_seen.add(step)\n",
        "                    sample_records.append(rec)\n",
        "                elif len(sample_records) < num_sample_records and step in steps_seen:\n",
        "                    # Replace one of the samples with a later step to spread steps\n",
        "                    pass  # keep first occurrence per step\n",
        "\n",
        "                lines_read += 1\n",
        "                total_lines += 1\n",
        "\n",
        "    return dict(step_records), sample_records, total_lines\n",
        "\n",
        "\n",
        "step_records, sample_records, total_lines = stream_rollouts(\n",
        "    paths,\n",
        "    step_min=STEP_MIN,\n",
        "    step_max=STEP_MAX,\n",
        "    max_lines_per_file=MAX_LINES_PER_FILE,\n",
        "    num_sample_records=NUM_SAMPLE_RECORDS,\n",
        ")\n",
        "print(f\"Total records read: {total_lines}\")\n",
        "print(f\"Steps with data: {len(step_records)}\")\n",
        "if step_records:\n",
        "    steps = sorted(step_records.keys())\n",
        "    print(f\"Step range: {steps[0]} .. {steps[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = paths[0]\n",
        "file = open(p)\n",
        "j = json.loads(file.readline())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(j['logprobs'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-step summary (rewards, lengths, logprobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_step_summary(step_records: dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for step in sorted(step_records.keys()):\n",
        "        stats = step_records[step]\n",
        "        n = len(stats)\n",
        "        rewards = [s[\"reward\"] for s in stats]\n",
        "        advantages = [s[\"advantage\"] for s in stats]\n",
        "        resp_lens = [s[\"response_len\"] for s in stats]\n",
        "        prompt_lens = [s[\"prompt_len\"] for s in stats]\n",
        "        finish_stop = sum(1 for s in stats if s[\"finish_reason\"] == \"stop\")\n",
        "\n",
        "        row = {\n",
        "            \"step\": step,\n",
        "            \"n\": n,\n",
        "            \"reward_mean\": np.mean(rewards),\n",
        "            \"reward_std\": np.std(rewards),\n",
        "            \"reward_min\": np.min(rewards),\n",
        "            \"reward_max\": np.max(rewards),\n",
        "            \"advantage_mean\": np.mean(advantages),\n",
        "            \"resp_len_mean\": np.mean(resp_lens),\n",
        "            \"resp_len_max\": np.max(resp_lens),\n",
        "            \"prompt_len_mean\": np.mean(prompt_lens),\n",
        "            \"stop_rate\": finish_stop / n if n else 0,\n",
        "        }\n",
        "        logprob_means = [s[\"logprob_mean\"] for s in stats if s.get(\"logprob_mean\") is not None]\n",
        "        if logprob_means:\n",
        "            row[\"logprob_mean_mean\"] = np.mean(logprob_means)\n",
        "            row[\"logprob_mean_std\"] = np.std(logprob_means)\n",
        "        else:\n",
        "            row[\"logprob_mean_mean\"] = np.nan\n",
        "            row[\"logprob_mean_std\"] = np.nan\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "df = build_step_summary(step_records)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot reward and response length vs step (if matplotlib available)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(df[\"step\"], df[\"reward_mean\"], label=\"reward mean\")\n",
        "    ax.fill_between(df[\"step\"], df[\"reward_min\"], df[\"reward_max\"], alpha=0.2)\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"reward\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(df[\"step\"], df[\"resp_len_mean\"], label=\"response length mean\")\n",
        "    ax.plot(df[\"step\"], df[\"resp_len_max\"], alpha=0.7, label=\"response length max\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"tokens\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(df[\"step\"], df[\"stop_rate\"], color=\"green\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"stop_rate\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    if df[\"logprob_mean_mean\"].notna().any():\n",
        "        ax.plot(df[\"step\"], df[\"logprob_mean_mean\"], label=\"mean logprob (vLLM)\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"logprob\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"matplotlib not available; skip plots.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample full records (token IDs only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, rec in enumerate(sample_records):\n",
        "    print(f\"--- Sample {i + 1} (step={rec.get('step')}, sample_idx={rec.get('sample_idx')}) ---\")\n",
        "    print(f\"  reward={rec.get('reward')}, advantage={rec.get('advantage')}, finish_reason={rec.get('finish_reason')}\")\n",
        "    print(f\"  prompt_tokens: len={len(rec.get('prompt_tokens', []))}\")\n",
        "    print(f\"  response_tokens: len={len(rec.get('response_tokens', []))}\")\n",
        "    if rec.get(\"logprobs\"):\n",
        "        lp = rec[\"logprobs\"]\n",
        "        valid = [x for x in lp if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "        if valid:\n",
        "            print(f\"  logprobs: len={len(lp)}, mean={np.mean(valid):.4f}, min={np.min(valid):.4f}, max={np.max(valid):.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optional: decode tokens to text (requires tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer from metadata model_name (or set MODEL_NAME explicitly)\n",
        "MODEL_NAME = (metadata or {}).get(\"model_name\", \"allenai/OLMo-1B-7B\")  # fallback\n",
        "USE_TOKENIZER = True  # set False to skip tokenizer load and decode\n",
        "\n",
        "if USE_TOKENIZER:\n",
        "    try:\n",
        "        from transformers import AutoTokenizer\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "        print(f\"Loaded tokenizer: {MODEL_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load tokenizer: {e}\")\n",
        "        tokenizer = None\n",
        "else:\n",
        "    tokenizer = None\n",
        "    print(\"Skipping tokenizer (USE_TOKENIZER=False).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(sample_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if tokenizer is not None and sample_records:\n",
        "    for i, rec in enumerate(sample_records[:3]):  # decode first 3 only\n",
        "        print(f\"=== Sample {i + 1} (step={rec.get('step')}) ===\")\n",
        "        prompt_ids = rec.get(\"prompt_tokens\", [])\n",
        "        response_ids = rec.get(\"response_tokens\", [])\n",
        "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
        "        response_text = tokenizer.decode(response_ids, skip_special_tokens=False)\n",
        "        print(\"[Prompt] (first 500 chars)\")\n",
        "        print(prompt_text[:500])\n",
        "        print(\"[Response] (first 800 chars)\")\n",
        "        print(response_text[:800])\n",
        "        print()\n",
        "else:\n",
        "    print(\"No tokenizer or no sample records; skip decode.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inspect a single step in detail (re-run with STEP_MIN/STEP_MAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: stream again with a narrow step range and larger sample to get many records from one step\n",
        "def stream_one_step(paths: list[str], target_step: int, max_records: int = 64):\n",
        "    records = []\n",
        "    for filepath in paths:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                rec = json.loads(line)\n",
        "                if rec.get(\"step\") == target_step:\n",
        "                    records.append(rec)\n",
        "                    if len(records) >= max_records:\n",
        "                        return records\n",
        "    return records\n",
        "\n",
        "\n",
        "TARGET_STEP = 30  # change to step of interest (e.g. 400, 500)\n",
        "one_step_records = stream_one_step(paths, TARGET_STEP)\n",
        "print(f\"Records at step {TARGET_STEP}: {len(one_step_records)}\")\n",
        "if one_step_records:\n",
        "    r0 = one_step_records[0]\n",
        "    print(f\"  reward: {r0.get('reward')}, advantage: {r0.get('advantage')}, finish_reason: {r0.get('finish_reason')}\")\n",
        "    print(f\"  response_tokens length: {len(r0.get('response_tokens', []))}\")\n",
        "    if r0.get(\"logprobs\"):\n",
        "        lp = [x for x in r0[\"logprobs\"] if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "        if lp:\n",
        "            print(f\"  logprobs: mean={np.mean(lp):.4f}, std={np.std(lp):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare first and last rollouts (2 each)\n",
        "\n",
        "def _tail_lines(path: str, n: int):\n",
        "    # read last n non-empty lines (binary-safe)\n",
        "    with open(path, 'rb') as f:\n",
        "        f.seek(0, 2)\n",
        "        size = f.tell()\n",
        "        block = bytearray()\n",
        "        lines = []\n",
        "        pos = size\n",
        "        while pos > 0 and len(lines) <= n:\n",
        "            toread = min(4096, pos)\n",
        "            pos -= toread\n",
        "            f.seek(pos)\n",
        "            data = f.read(toread)\n",
        "            block = data + block\n",
        "            lines = block.splitlines()\n",
        "        return [l.decode('utf-8', 'ignore') for l in lines[-n:]]\n",
        "\n",
        "\n",
        "def get_first_last_rollouts(paths: list[str], n_each: int = 2):\n",
        "    first = []\n",
        "    last = []\n",
        "    # collect first records from files in order\n",
        "    for p in paths:\n",
        "        with open(p) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                first.append(json.loads(line))\n",
        "                if len(first) >= n_each:\n",
        "                    break\n",
        "        if len(first) >= n_each:\n",
        "            break\n",
        "    # collect last records from files in reverse order\n",
        "    for p in reversed(paths):\n",
        "        try:\n",
        "            tail = _tail_lines(p, n_each * 2)  # read a few to skip blanks\n",
        "        except Exception:\n",
        "            tail = []\n",
        "        for line in reversed(tail):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                last.append(json.loads(line))\n",
        "            except Exception:\n",
        "                continue\n",
        "            if len(last) >= n_each:\n",
        "                break\n",
        "        if len(last) >= n_each:\n",
        "            break\n",
        "    last = list(reversed(last))  # keep chronological order for the late set\n",
        "    return first, last\n",
        "\n",
        "\n",
        "def summarize_and_decode(records: list[dict], label: str, tokenizer=None, max_chars=800):\n",
        "    print(f'=== {label} ({len(records)} records) ===')\n",
        "    for i, r in enumerate(records):\n",
        "        print(f'-- {label} #{i+1}: step={r.get(\"step\")}, sample_idx={r.get(\"sample_idx\")}')\n",
        "        print(f'   reward={r.get(\"reward\")}, advantage={r.get(\"advantage\")}, finish_reason={r.get(\"finish_reason\")}')\n",
        "        prompt_ids = r.get('prompt_tokens', [])\n",
        "        resp_ids = r.get('response_tokens', [])\n",
        "        print(f'   prompt_len={len(prompt_ids)}, response_len={len(resp_ids)}')\n",
        "        lp = r.get('logprobs')\n",
        "        if lp:\n",
        "            valid = [x for x in lp if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "            if valid:\n",
        "                print(f'   logprobs: n={len(lp)}, mean={np.mean(valid):.4f}, std={np.std(valid):.4f}')\n",
        "        if tokenizer is not None:\n",
        "            try:\n",
        "                ptext = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
        "                rtext = tokenizer.decode(resp_ids, skip_special_tokens=False)\n",
        "                print('   [Prompt] (first 300 chars)')\n",
        "                print(ptext[:].replace('\\n', ' '))\n",
        "                print('   [Response] (first 600 chars)')\n",
        "                print(rtext[:].replace('\\n', ' '))\n",
        "            except Exception as e:\n",
        "                print(f'   decode failed: {e}')\n",
        "        print()\n",
        "\n",
        "\n",
        "# Run comparison (2 earliest, 2 latest)\n",
        "early, late = get_first_last_rollouts(paths, n_each=2)\n",
        "# load tokenizer if available (reuse earlier `tokenizer` variable if set)\n",
        "tk = globals().get('tokenizer', None)\n",
        "summarize_and_decode(early, 'EARLY', tokenizer=tk)\n",
        "summarize_and_decode(late, 'LATE', tokenizer=tk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print one (or more) task-response examples per step (memory-efficient)\n",
        "\n",
        "def load_examples_by_step(paths: list[str], steps: list[int] | None = None, max_examples_per_step: int = 1):\n",
        "    \"\"\"Load up to `max_examples_per_step` examples per step from the rollouts.\"\"\"\n",
        "    examples = defaultdict(list)  # step -> list[rec]\n",
        "\n",
        "    for filepath in paths:\n",
        "        try:\n",
        "            with open(filepath) as f:\n",
        "                for raw in f:\n",
        "                    line = raw.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        rec = json.loads(line)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    step = rec.get('step')\n",
        "                    if step is None:\n",
        "                        continue\n",
        "                    if steps and step not in steps:\n",
        "                        continue\n",
        "                    if len(examples[step]) < max_examples_per_step:\n",
        "                        examples[step].append(rec)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return dict(examples)\n",
        "\n",
        "\n",
        "paths = sorted(paths)[-1:]\n",
        "\n",
        "examples = load_examples_by_step(paths, steps=None, max_examples_per_step=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_examples(examples: dict, tokenizer):\n",
        "    for step in sorted(examples.keys()):\n",
        "        # if step % 10 == 0:\n",
        "        for i, r in enumerate(examples[step]):\n",
        "            pids = r.get('prompt_tokens', [])\n",
        "            rids = r.get('response_tokens', [])\n",
        "            print(f'=== STEP {step}  ({i+1}/{len(examples[step])}) - dataset={r.get(\"dataset\")} ===')\n",
        "            print(f\"  reward={round(r.get('reward', 0), 2)}, advantage={round(r.get('advantage', 0), 2)}, finish_reason={r.get('finish_reason')},  prompt_len={len(pids)}, response_len={len(rids)}\")\n",
        "            try:\n",
        "                ptext = tokenizer.decode(pids, skip_special_tokens=False)\n",
        "                rtext = tokenizer.decode(rids, skip_special_tokens=False)\n",
        "                print('  [Prompt]')\n",
        "                print('   ', ptext[:].replace('\\n', ' '))\n",
        "                print('  [Response]')\n",
        "                print('   ', rtext[:].replace('\\n', ' '))\n",
        "            except Exception as e:\n",
        "                print('  decode failed:', e)\n",
        "steps = sorted(examples.keys())\n",
        "recent_steps = steps[:]\n",
        "# Run the per-step printer with default options (prints 1 example per step)\n",
        "print_examples(tokenizer=globals().get('tokenizer', None), examples={step: examples[step] for step in recent_steps})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples[328][0].keys()\n",
        "for k in ['step', 'sample_idx', 'prompt_idx', 'reward', 'advantage', 'finish_reason', 'dataset', 'ground_truth', 'request_info']:\n",
        "    print(f\"{k}: {examples[328][0].get(k)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Diagnose vLLM vs Local Logprob Divergence\n",
        "\n",
        "**Goal:** Check whether the logprob divergence is caused by prompt-response misalignment (wrong responses mapped to wrong prompts) or by genuine model drift.\n",
        "\n",
        "Checks performed:\n",
        "1. **Prompt consistency within groups:** All `num_samples_per_prompt` samples sharing the same `prompt_idx` at a given step must have identical `prompt_tokens`.\n",
        "2. **Logprob length matches response length:** `len(logprobs) == len(response_tokens)` for every record.\n",
        "3. **Logprob NaN/invalid fraction:** Fraction of NaN or missing logprobs per step.\n",
        "4. **Local forward pass vs vLLM logprobs:** Load model, run a forward pass on saved token IDs, and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Check 1 & 2: Prompt-response alignment + logprob length consistency ---\n",
        "# Streams ALL rollout files for the current run and reports:\n",
        "# - Whether prompt_tokens are identical for records sharing (step, prompt_idx)\n",
        "# - Whether len(logprobs) == len(response_tokens) for every record\n",
        "# - NaN fraction in logprobs per step\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "alignment_errors = []\n",
        "logprob_len_mismatches = []\n",
        "step_nan_fractions = defaultdict(list)  # step -> list of nan_fraction per sample\n",
        "step_logprob_means = defaultdict(list)\n",
        "total_records = 0\n",
        "prompt_groups = defaultdict(dict)  # (step, prompt_idx) -> {sample_idx: prompt_tokens_hash}\n",
        "\n",
        "for filepath in paths:\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            total_records += 1\n",
        "            step = rec['step']\n",
        "            pidx = rec['prompt_idx']\n",
        "            sidx = rec['sample_idx']\n",
        "            pt = rec['prompt_tokens']\n",
        "            rt = rec['response_tokens']\n",
        "            lp = rec.get('logprobs', [])\n",
        "\n",
        "            # Check 1: prompt consistency within groups\n",
        "            group_key = (step, pidx)\n",
        "            pt_hash = hash(tuple(pt))\n",
        "            if group_key not in prompt_groups:\n",
        "                prompt_groups[group_key] = {'hash': pt_hash, 'first_sidx': sidx, 'first_pt_head': pt[:5]}\n",
        "            elif prompt_groups[group_key]['hash'] != pt_hash:\n",
        "                alignment_errors.append({\n",
        "                    'step': step, 'prompt_idx': pidx, 'sample_idx': sidx,\n",
        "                    'expected_head': prompt_groups[group_key]['first_pt_head'],\n",
        "                    'got_head': pt[:5],\n",
        "                    'first_sidx': prompt_groups[group_key]['first_sidx'],\n",
        "                })\n",
        "\n",
        "            # Check 2: logprob length matches response length\n",
        "            if lp is not None and len(lp) != len(rt):\n",
        "                logprob_len_mismatches.append({\n",
        "                    'step': step, 'sample_idx': sidx,\n",
        "                    'resp_len': len(rt), 'lp_len': len(lp),\n",
        "                })\n",
        "\n",
        "            # Check 3: NaN fraction\n",
        "            if lp:\n",
        "                nan_count = sum(1 for x in lp if x is None or (isinstance(x, float) and np.isnan(x)))\n",
        "                step_nan_fractions[step].append(nan_count / len(lp) if len(lp) > 0 else 0)\n",
        "                valid = [x for x in lp if x is not None and not (isinstance(x, float) and np.isnan(x))]\n",
        "                if valid:\n",
        "                    step_logprob_means[step].append(float(np.mean(valid)))\n",
        "\n",
        "print(f\"Total records scanned: {total_records}\")\n",
        "print(f\"Unique (step, prompt_idx) groups: {len(prompt_groups)}\")\n",
        "print()\n",
        "\n",
        "# Report Check 1\n",
        "if alignment_errors:\n",
        "    print(f\"❌ PROMPT ALIGNMENT ERRORS: {len(alignment_errors)}\")\n",
        "    for err in alignment_errors[:5]:\n",
        "        print(f\"   step={err['step']}, prompt_idx={err['prompt_idx']}, \"\n",
        "              f\"sample_idx={err['sample_idx']} vs first_sidx={err['first_sidx']}\")\n",
        "        print(f\"   expected prompt[:5]={err['expected_head']}, got={err['got_head']}\")\n",
        "else:\n",
        "    print(\"✅ Prompt alignment: All samples within each (step, prompt_idx) group have identical prompt_tokens.\")\n",
        "\n",
        "# Report Check 2\n",
        "if logprob_len_mismatches:\n",
        "    print(f\"\\n❌ LOGPROB LENGTH MISMATCHES: {len(logprob_len_mismatches)}\")\n",
        "    for mm in logprob_len_mismatches[:5]:\n",
        "        print(f\"   step={mm['step']}, sample_idx={mm['sample_idx']}: resp_len={mm['resp_len']}, lp_len={mm['lp_len']}\")\n",
        "else:\n",
        "    print(\"✅ Logprob lengths: len(logprobs) == len(response_tokens) for all records.\")\n",
        "\n",
        "# Report Check 3\n",
        "print(f\"\\n--- NaN fraction in vLLM logprobs per step (sample of every 50 steps) ---\")\n",
        "for step in sorted(step_nan_fractions.keys()):\n",
        "    if step % 50 == 0:\n",
        "        fracs = step_nan_fractions[step]\n",
        "        print(f\"  step={step:5d}: mean_nan_frac={np.mean(fracs):.4f}, max_nan_frac={np.max(fracs):.4f}, n={len(fracs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plot vLLM logprob mean over training steps ---\n",
        "# This shows if vLLM logprobs change as training progresses (they should become\n",
        "# slightly different from local logprobs as the learner model drifts between weight syncs).\n",
        "\n",
        "steps_sorted = sorted(step_logprob_means.keys())\n",
        "lp_mean_per_step = [np.mean(step_logprob_means[s]) for s in steps_sorted]\n",
        "lp_std_per_step = [np.std(step_logprob_means[s]) for s in steps_sorted]\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "    ax.plot(steps_sorted, lp_mean_per_step, label='vLLM logprob mean (per step)')\n",
        "    ax.fill_between(steps_sorted,\n",
        "                     [m - s for m, s in zip(lp_mean_per_step, lp_std_per_step)],\n",
        "                     [m + s for m, s in zip(lp_mean_per_step, lp_std_per_step)],\n",
        "                     alpha=0.2)\n",
        "    ax.set_xlabel('step')\n",
        "    ax.set_ylabel('mean logprob')\n",
        "    ax.set_title('vLLM logprob mean across training steps')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"matplotlib not available; printing table instead.\")\n",
        "    for s, m, sd in zip(steps_sorted, lp_mean_per_step, lp_std_per_step):\n",
        "        if s % 50 == 0:\n",
        "            print(f\"  step={s:5d}: lp_mean={m:.4f}, lp_std={sd:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Local forward pass vs vLLM logprobs\n",
        "\n",
        "Load the **base model** (before any training), run a forward pass on the saved `prompt_tokens + response_tokens`, and compare the local logprobs with the stored vLLM logprobs.\n",
        "\n",
        "If at step 0 the logprobs match closely (< 0.01 mean abs diff), the prompt-response mapping is correct. If they diverge even at step 0, something is wrong with how tokens are saved or how logprobs are extracted.\n",
        "\n",
        "**Note:** This requires a GPU. The model loaded here is the *initial* checkpoint, so it should match vLLM logprobs at early steps. At later steps, the learner model will have drifted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Local forward pass comparison with vLLM logprobs ---\n",
        "# Pick a few samples from step 0 (where model = initial weights = vLLM weights) and\n",
        "# a few from a later step to compare.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = (metadata or {}).get(\"model_name\", \"allenai/Olmo-3-7B-Think-DPO\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_COMPARE = 5  # number of samples to compare\n",
        "TEMPERATURE = 1.0  # must match the generation temperature used during rollout\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME} on {DEVICE}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
        ").to(DEVICE).eval()\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "print(f\"Model loaded. Vocab size: {model.config.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Compute local logprobs and compare with vLLM logprobs ---\n",
        "\n",
        "def compute_local_logprobs(model, input_ids: list[int], device: str, temperature: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"Run a forward pass and extract per-token logprobs for the response portion.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        ids = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
        "        outputs = model(ids)\n",
        "        logits = outputs.logits[0]  # (seq_len, vocab_size)\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "        # logprob of each token given the previous context: log_probs[t-1, token_t]\n",
        "        # So for position t (0-indexed), the logprob is log_probs[t-1, input_ids[t]]\n",
        "        token_logprobs = []\n",
        "        for t in range(1, len(input_ids)):\n",
        "            token_logprobs.append(log_probs[t - 1, input_ids[t]].item())\n",
        "    return np.array(token_logprobs)\n",
        "\n",
        "\n",
        "def compare_logprobs_for_records(records: list[dict], model, device: str, temperature: float = 1.0, label: str = \"\"):\n",
        "    \"\"\"Compare local vs vLLM logprobs for a list of rollout records.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Comparing local vs vLLM logprobs: {label} ({len(records)} samples)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    all_diffs = []\n",
        "    for i, rec in enumerate(records):\n",
        "        prompt_ids = rec['prompt_tokens']\n",
        "        resp_ids = rec['response_tokens']\n",
        "        vllm_lp = rec.get('logprobs', [])\n",
        "\n",
        "        if not vllm_lp or not resp_ids:\n",
        "            print(f\"  Sample {i}: skipped (no logprobs or empty response)\")\n",
        "            continue\n",
        "\n",
        "        full_ids = prompt_ids + resp_ids\n",
        "        local_lp_full = compute_local_logprobs(model, full_ids, device, temperature)\n",
        "\n",
        "        # local_lp_full has logprobs for positions 1..len(full_ids)-1\n",
        "        # vLLM logprobs correspond to response tokens only\n",
        "        prompt_len = len(prompt_ids)\n",
        "        # local logprobs for response tokens: positions prompt_len..end\n",
        "        # In local_lp_full, index (prompt_len - 1) corresponds to logprob of token at position prompt_len\n",
        "        local_lp_response = local_lp_full[prompt_len - 1:]  # logprobs for resp tokens\n",
        "\n",
        "        vllm_lp_arr = np.array(vllm_lp, dtype=np.float64)\n",
        "        # Filter out NaN vllm logprobs\n",
        "        valid_mask = ~np.isnan(vllm_lp_arr)\n",
        "\n",
        "        if len(local_lp_response) != len(vllm_lp_arr):\n",
        "            print(f\"  Sample {i}: LENGTH MISMATCH local={len(local_lp_response)} vs vllm={len(vllm_lp_arr)}\")\n",
        "            continue\n",
        "\n",
        "        diffs = np.abs(local_lp_response[valid_mask] - vllm_lp_arr[valid_mask])\n",
        "        all_diffs.extend(diffs.tolist())\n",
        "\n",
        "        mean_diff = diffs.mean()\n",
        "        max_diff = diffs.max()\n",
        "        median_diff = np.median(diffs)\n",
        "\n",
        "        # Check first few token logprobs in detail\n",
        "        print(f\"  Sample {i} (step={rec.get('step')}, sidx={rec.get('sample_idx')}): \"\n",
        "              f\"mean_abs_diff={mean_diff:.6f}, max_abs_diff={max_diff:.6f}, median={median_diff:.6f}, \"\n",
        "              f\"resp_len={len(resp_ids)}\")\n",
        "\n",
        "        # Show first 5 token comparisons\n",
        "        for t in range(min(5, len(resp_ids))):\n",
        "            if valid_mask[t]:\n",
        "                token_text = tok.decode([resp_ids[t]])\n",
        "                print(f\"    token[{t}]={resp_ids[t]:6d} ({token_text!r:>12s}): \"\n",
        "                      f\"local={local_lp_response[t]:.6f}, vllm={vllm_lp_arr[t]:.6f}, \"\n",
        "                      f\"diff={abs(local_lp_response[t] - vllm_lp_arr[t]):.6f}\")\n",
        "\n",
        "    if all_diffs:\n",
        "        all_diffs = np.array(all_diffs)\n",
        "        print(f\"\\n  OVERALL: mean_abs_diff={all_diffs.mean():.6f}, max={all_diffs.max():.6f}, \"\n",
        "              f\"median={np.median(all_diffs):.6f}, p95={np.percentile(all_diffs, 95):.6f}, \"\n",
        "              f\"p99={np.percentile(all_diffs, 99):.6f}\")\n",
        "        if all_diffs.mean() < 0.05:\n",
        "            print(\"  ✅ Logprobs match closely — prompt-response alignment looks correct.\")\n",
        "        elif all_diffs.mean() < 0.5:\n",
        "            print(\"  ⚠️  Small divergence — likely due to bf16/fp16 precision differences or temperature.\")\n",
        "        else:\n",
        "            print(\"  ❌ Large divergence — possible prompt-response misalignment or model mismatch!\")\n",
        "\n",
        "\n",
        "# Collect samples from step 0 (earliest) and a late step\n",
        "step0_records = []\n",
        "late_step_records = []\n",
        "latest_step = max(step_logprob_means.keys()) if step_logprob_means else 0\n",
        "\n",
        "for filepath in paths:\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            if rec['step'] == 0 and len(step0_records) < NUM_COMPARE:\n",
        "                step0_records.append(rec)\n",
        "            if rec['step'] == latest_step and len(late_step_records) < NUM_COMPARE:\n",
        "                late_step_records.append(rec)\n",
        "            if len(step0_records) >= NUM_COMPARE and len(late_step_records) >= NUM_COMPARE:\n",
        "                break\n",
        "    if len(step0_records) >= NUM_COMPARE and len(late_step_records) >= NUM_COMPARE:\n",
        "        break\n",
        "\n",
        "print(f\"Collected {len(step0_records)} step-0 records, {len(late_step_records)} step-{latest_step} records\")\n",
        "\n",
        "# Compare step 0 (should match closely since model = initial weights = vLLM weights)\n",
        "compare_logprobs_for_records(step0_records, model, DEVICE, TEMPERATURE, label=f\"Step 0 (initial model)\")\n",
        "\n",
        "# Compare latest step (expected to diverge more since learner has trained)\n",
        "compare_logprobs_for_records(late_step_records, model, DEVICE, TEMPERATURE, label=f\"Step {latest_step} (latest)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Response-prompt coherence check\n",
        "\n",
        "An additional check: decode a few prompt-response pairs and verify that the response is actually a plausible continuation of the prompt (not a response to a different question). This catches subtle misalignment that token-level checks might miss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Response-prompt coherence: eyeball check with decoded text ---\n",
        "# Also checks: within a (step, prompt_idx) group, are all responses different?\n",
        "# (If responses are identical across samples, something might be wrong with sampling.)\n",
        "\n",
        "def check_response_diversity_and_coherence(\n",
        "    paths: list[str], steps_to_check: list[int], n_groups: int = 3, tokenizer=None\n",
        "):\n",
        "    \"\"\"For each step, pick a few prompt groups and:\n",
        "    1. Show decoded prompt + response for visual inspection.\n",
        "    2. Check if responses within a group are diverse (not duplicated).\n",
        "    \"\"\"\n",
        "    for target_step in steps_to_check:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Step {target_step}: Coherence & Diversity Check\")\n",
        "        print(f\"{'='*60}\")\n",
        "        groups = defaultdict(list)  # prompt_idx -> [records]\n",
        "        for filepath in paths:\n",
        "            with open(filepath) as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    rec = json.loads(line)\n",
        "                    if rec['step'] == target_step:\n",
        "                        groups[rec['prompt_idx']].append(rec)\n",
        "            if len(groups) >= n_groups + 2:\n",
        "                break\n",
        "\n",
        "        if not groups:\n",
        "            print(f\"  No records found for step {target_step}\")\n",
        "            continue\n",
        "\n",
        "        for pidx in sorted(groups.keys())[:n_groups]:\n",
        "            recs = groups[pidx]\n",
        "            print(f\"\\n  --- prompt_idx={pidx}, n_samples={len(recs)} ---\")\n",
        "\n",
        "            # Decode prompt (same for all in group)\n",
        "            if tokenizer:\n",
        "                prompt_text = tokenizer.decode(recs[0]['prompt_tokens'], skip_special_tokens=False)\n",
        "                print(f\"  [Prompt] (first 400 chars)\")\n",
        "                print(f\"  {prompt_text[:400]}\")\n",
        "            print(f\"  [Ground truth]: {str(recs[0].get('ground_truth', ''))[:200]}\")\n",
        "\n",
        "            # Check response diversity\n",
        "            resp_hashes = set()\n",
        "            for i, r in enumerate(recs[:6]):\n",
        "                rt_hash = hash(tuple(r['response_tokens']))\n",
        "                resp_hashes.add(rt_hash)\n",
        "                if tokenizer:\n",
        "                    resp_text = tokenizer.decode(r['response_tokens'], skip_special_tokens=False)\n",
        "                    print(f\"  [Response {i}] reward={r['reward']:.2f}, len={len(r['response_tokens'])}, \"\n",
        "                          f\"finish={r['finish_reason']}\")\n",
        "                    print(f\"    {resp_text[:300]}\")\n",
        "\n",
        "            if len(resp_hashes) < len(recs[:6]):\n",
        "                print(f\"  ⚠️  Only {len(resp_hashes)} unique responses out of {len(recs[:6])} samples!\")\n",
        "            else:\n",
        "                print(f\"  ✅ All {len(recs[:6])} sampled responses are unique.\")\n",
        "\n",
        "\n",
        "# Check early, middle, and late steps\n",
        "all_steps = sorted(step_logprob_means.keys())\n",
        "mid_step = all_steps[len(all_steps) // 2] if all_steps else 0\n",
        "check_steps = [0, mid_step, all_steps[-1]] if all_steps else [0]\n",
        "check_steps = sorted(set(check_steps))\n",
        "\n",
        "tk = globals().get('tokenizer', tok if 'tok' in dir() else None)\n",
        "check_response_diversity_and_coherence(paths, check_steps, n_groups=2, tokenizer=tk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Off-by-one check: first response token logprob\n",
        "\n",
        "vLLM can sometimes return N-1 logprobs for N response tokens (missing the first token's logprob). Check if the first logprob in each record is NaN (indicating it was synthesized) vs a real value. Also check if shifting the vLLM logprobs by 1 gives a better match with local logprobs (would indicate an off-by-one in the alignment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Off-by-one check ---\n",
        "# If vLLM logprobs are shifted by 1 relative to local logprobs, shifting them would\n",
        "# reduce the disagreement. We test:\n",
        "#   1) Normal alignment: local_lp[prompt_len-1:] vs vllm_lp\n",
        "#   2) Shifted by +1:    local_lp[prompt_len:] vs vllm_lp[:-1]  (vLLM is ahead by 1)\n",
        "#   3) Shifted by -1:    local_lp[prompt_len-2:] vs vllm_lp[1:]  (vLLM is behind by 1)\n",
        "\n",
        "def check_offbyone(records: list[dict], model, device: str, temperature: float = 1.0, label: str = \"\"):\n",
        "    print(f\"\\n--- Off-by-one check: {label} ---\")\n",
        "    results = {'normal': [], 'shift+1': [], 'shift-1': []}\n",
        "\n",
        "    for rec in records:\n",
        "        prompt_ids = rec['prompt_tokens']\n",
        "        resp_ids = rec['response_tokens']\n",
        "        vllm_lp = rec.get('logprobs', [])\n",
        "        if not vllm_lp or len(resp_ids) < 5:\n",
        "            continue\n",
        "\n",
        "        full_ids = prompt_ids + resp_ids\n",
        "        local_lp_full = compute_local_logprobs(model, full_ids, device, temperature)\n",
        "        prompt_len = len(prompt_ids)\n",
        "        vllm_arr = np.array(vllm_lp, dtype=np.float64)\n",
        "        valid = ~np.isnan(vllm_arr)\n",
        "\n",
        "        # Normal alignment\n",
        "        local_resp = local_lp_full[prompt_len - 1:]\n",
        "        if len(local_resp) == len(vllm_arr):\n",
        "            diffs = np.abs(local_resp[valid] - vllm_arr[valid])\n",
        "            results['normal'].append(diffs.mean())\n",
        "\n",
        "        # Shift +1: compare local[prompt_len:] with vllm[:-1]\n",
        "        local_shifted_plus = local_lp_full[prompt_len:]\n",
        "        vllm_shifted_minus = vllm_arr[:-1]\n",
        "        valid_s = ~np.isnan(vllm_shifted_minus)\n",
        "        min_len = min(len(local_shifted_plus), len(vllm_shifted_minus))\n",
        "        if min_len > 0:\n",
        "            diffs_p = np.abs(local_shifted_plus[:min_len][valid_s[:min_len]] - vllm_shifted_minus[:min_len][valid_s[:min_len]])\n",
        "            if len(diffs_p) > 0:\n",
        "                results['shift+1'].append(diffs_p.mean())\n",
        "\n",
        "        # Shift -1: compare local[prompt_len-2:] with vllm[1:]\n",
        "        if prompt_len >= 2:\n",
        "            local_shifted_minus = local_lp_full[prompt_len - 2:]\n",
        "            vllm_shifted_plus = vllm_arr[1:]\n",
        "            valid_m = ~np.isnan(vllm_shifted_plus)\n",
        "            min_len = min(len(local_shifted_minus), len(vllm_shifted_plus))\n",
        "            if min_len > 0:\n",
        "                diffs_m = np.abs(local_shifted_minus[:min_len][valid_m[:min_len]] - vllm_shifted_plus[:min_len][valid_m[:min_len]])\n",
        "                if len(diffs_m) > 0:\n",
        "                    results['shift-1'].append(diffs_m.mean())\n",
        "\n",
        "    for key in ['normal', 'shift+1', 'shift-1']:\n",
        "        if results[key]:\n",
        "            print(f\"  {key:>10s}: mean_abs_diff={np.mean(results[key]):.6f} (over {len(results[key])} samples)\")\n",
        "        else:\n",
        "            print(f\"  {key:>10s}: no data\")\n",
        "\n",
        "    # Determine best alignment\n",
        "    means = {k: np.mean(v) if v else float('inf') for k, v in results.items()}\n",
        "    best = min(means, key=means.get)\n",
        "    if best == 'normal':\n",
        "        print(\"  ✅ Normal alignment is best — no off-by-one issue detected.\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  '{best}' alignment gives lower error — possible off-by-one in logprob indexing!\")\n",
        "\n",
        "    # Also check: is the first logprob typically NaN?\n",
        "    first_lp_nan = sum(1 for rec in records if rec.get('logprobs') and np.isnan(rec['logprobs'][0]))\n",
        "    first_lp_valid = sum(1 for rec in records if rec.get('logprobs') and not np.isnan(rec['logprobs'][0]))\n",
        "    print(f\"  First response token logprob: NaN in {first_lp_nan}/{first_lp_nan + first_lp_valid} records\")\n",
        "\n",
        "\n",
        "check_offbyone(step0_records, model, DEVICE, TEMPERATURE, label=\"Step 0\")\n",
        "if late_step_records:\n",
        "    check_offbyone(late_step_records, model, DEVICE, TEMPERATURE, label=f\"Step {latest_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 Per-token logprob diff distribution (heatmap for one sample)\n",
        "\n",
        "Visualize the per-token absolute difference between local and vLLM logprobs across the full response. Helps spot if the divergence is concentrated at certain positions (e.g., beginning, end, or at tool call boundaries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Per-token logprob diff plot for step 0 samples ---\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, axes = plt.subplots(min(len(step0_records), 3), 1, figsize=(14, 3 * min(len(step0_records), 3)), squeeze=False)\n",
        "    for idx, rec in enumerate(step0_records[:3]):\n",
        "        prompt_ids = rec['prompt_tokens']\n",
        "        resp_ids = rec['response_tokens']\n",
        "        vllm_lp = np.array(rec.get('logprobs', []), dtype=np.float64)\n",
        "        if len(vllm_lp) == 0:\n",
        "            continue\n",
        "\n",
        "        full_ids = prompt_ids + resp_ids\n",
        "        local_lp_full = compute_local_logprobs(model, full_ids, DEVICE, TEMPERATURE)\n",
        "        prompt_len = len(prompt_ids)\n",
        "        local_lp_resp = local_lp_full[prompt_len - 1:]\n",
        "\n",
        "        valid = ~np.isnan(vllm_lp)\n",
        "        positions = np.arange(len(vllm_lp))\n",
        "\n",
        "        ax = axes[idx, 0]\n",
        "        # Plot both logprob curves\n",
        "        ax.plot(positions[valid], local_lp_resp[valid], alpha=0.7, label='local', linewidth=0.5)\n",
        "        ax.plot(positions[valid], vllm_lp[valid], alpha=0.7, label='vLLM', linewidth=0.5)\n",
        "        ax.set_xlabel('response token position')\n",
        "        ax.set_ylabel('logprob')\n",
        "        ax.set_title(f'Sample {idx} (step={rec[\"step\"]}, sidx={rec[\"sample_idx\"]}): '\n",
        "                      f'mean_abs_diff={np.abs(local_lp_resp[valid] - vllm_lp[valid]).mean():.4f}')\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Also plot the absolute difference\n",
        "    fig2, axes2 = plt.subplots(min(len(step0_records), 3), 1, figsize=(14, 2 * min(len(step0_records), 3)), squeeze=False)\n",
        "    for idx, rec in enumerate(step0_records[:3]):\n",
        "        prompt_ids = rec['prompt_tokens']\n",
        "        resp_ids = rec['response_tokens']\n",
        "        vllm_lp = np.array(rec.get('logprobs', []), dtype=np.float64)\n",
        "        if len(vllm_lp) == 0:\n",
        "            continue\n",
        "\n",
        "        full_ids = prompt_ids + resp_ids\n",
        "        local_lp_full = compute_local_logprobs(model, full_ids, DEVICE, TEMPERATURE)\n",
        "        prompt_len = len(prompt_ids)\n",
        "        local_lp_resp = local_lp_full[prompt_len - 1:]\n",
        "\n",
        "        valid = ~np.isnan(vllm_lp)\n",
        "        positions = np.arange(len(vllm_lp))\n",
        "        abs_diff = np.abs(local_lp_resp - vllm_lp)\n",
        "\n",
        "        ax = axes2[idx, 0]\n",
        "        ax.bar(positions[valid], abs_diff[valid], width=1.0, alpha=0.7, color='red')\n",
        "        ax.set_xlabel('response token position')\n",
        "        ax.set_ylabel('|local - vLLM|')\n",
        "        ax.set_title(f'Abs diff sample {idx}')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(\"matplotlib not available; skip plots.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5 Parse training log divergence metrics\n",
        "\n",
        "Extract `vllm_vs_local_logprob_diff_mean` from the training log file and plot it vs training step to see exactly when the divergence begins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Parse training log to extract divergence metrics ---\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "LOG_FILE = \"/mnt/vast/home/lh22zyta/shortcut-RL/open-instruct/logs/RLVR-soofi-Olmo_IsomorphicRL_86561.out\"\n",
        "\n",
        "def parse_training_log(log_file: str) -> pd.DataFrame:\n",
        "    \"\"\"Extract training_step and debug metrics from the log file.\"\"\"\n",
        "    steps = []\n",
        "    diffs = []\n",
        "    diff_maxes = []\n",
        "    losses = []\n",
        "    ws_maxes = []\n",
        "\n",
        "    step_re = re.compile(r'training_step:\\s*(\\d+)')\n",
        "    diff_re = re.compile(r'vllm_vs_local_logprob_diff_mean:\\s*([\\d.e+-]+)')\n",
        "    diff_max_re = re.compile(r'vllm_vs_local_logprob_diff_max:\\s*([\\d.e+-]+)')\n",
        "    loss_re = re.compile(r'policy_avg:\\s*([\\d.e+-]+)')\n",
        "    ws_re = re.compile(r'weight_sync_max:\\s*([\\d.e+-]+)')\n",
        "\n",
        "    with open(log_file) as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split by metrics blocks\n",
        "    step_matches = step_re.findall(content)\n",
        "    diff_matches = diff_re.findall(content)\n",
        "    diff_max_matches = diff_max_re.findall(content)\n",
        "    loss_matches = loss_re.findall(content)\n",
        "    ws_matches = ws_re.findall(content)\n",
        "\n",
        "    n = min(len(step_matches), len(diff_matches))\n",
        "    rows = []\n",
        "    for i in range(n):\n",
        "        row = {\n",
        "            'training_step': int(step_matches[i]),\n",
        "            'logprob_diff_mean': float(diff_matches[i]),\n",
        "        }\n",
        "        if i < len(diff_max_matches):\n",
        "            row['logprob_diff_max'] = float(diff_max_matches[i])\n",
        "        if i < len(loss_matches):\n",
        "            row['policy_loss'] = float(loss_matches[i])\n",
        "        if i < len(ws_matches):\n",
        "            row['weight_sync_max'] = float(ws_matches[i])\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "if os.path.exists(LOG_FILE):\n",
        "    log_df = parse_training_log(LOG_FILE)\n",
        "    print(f\"Parsed {len(log_df)} training steps from log\")\n",
        "    display(log_df.describe())\n",
        "\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "        ax = axes[0, 0]\n",
        "        ax.plot(log_df['training_step'], log_df['logprob_diff_mean'], linewidth=0.8)\n",
        "        ax.set_xlabel('training step')\n",
        "        ax.set_ylabel('logprob diff mean')\n",
        "        ax.set_title('vLLM vs Local Logprob Diff (Mean)')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        # Mark the divergence onset\n",
        "        threshold = 0.5\n",
        "        diverge_steps = log_df[log_df['logprob_diff_mean'] > threshold]\n",
        "        if len(diverge_steps) > 0:\n",
        "            first_diverge = diverge_steps.iloc[0]['training_step']\n",
        "            ax.axvline(x=first_diverge, color='red', linestyle='--', alpha=0.7,\n",
        "                       label=f'First diff > {threshold} at step {int(first_diverge)}')\n",
        "            ax.legend()\n",
        "\n",
        "        ax = axes[0, 1]\n",
        "        if 'logprob_diff_max' in log_df.columns:\n",
        "            ax.plot(log_df['training_step'], log_df['logprob_diff_max'], linewidth=0.8, color='orange')\n",
        "            ax.set_xlabel('training step')\n",
        "            ax.set_ylabel('logprob diff max')\n",
        "            ax.set_title('vLLM vs Local Logprob Diff (Max)')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        ax = axes[1, 0]\n",
        "        if 'policy_loss' in log_df.columns:\n",
        "            ax.plot(log_df['training_step'], log_df['policy_loss'], linewidth=0.8, color='green')\n",
        "            ax.set_xlabel('training step')\n",
        "            ax.set_ylabel('policy loss')\n",
        "            ax.set_title('Policy Loss')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        ax = axes[1, 1]\n",
        "        if 'weight_sync_max' in log_df.columns:\n",
        "            ax.plot(log_df['training_step'], log_df['weight_sync_max'], linewidth=0.8, color='purple')\n",
        "            ax.set_xlabel('training step')\n",
        "            ax.set_ylabel('weight sync max (s)')\n",
        "            ax.set_title('Weight Sync Time (Max)')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"matplotlib not available; skip plots\")\n",
        "else:\n",
        "    print(f\"Log file not found: {LOG_FILE}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "open-instruct (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
