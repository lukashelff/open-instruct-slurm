{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspect GRPO rollout traces\n",
        "\n",
        "Rollouts are saved as **tokenized** JSONL: one JSON object per line with `prompt_tokens`, `response_tokens` (token IDs), and optionally `logprobs` (vLLM logprobs per response token). Files can be very large (multi-GB). This notebook streams them line-by-line to avoid loading everything into memory.\n",
        "\n",
        "**RolloutRecord fields:** `step`, `sample_idx`, `prompt_idx`, `prompt_tokens`, `response_tokens`, `reward`, `advantage`, `finish_reason`, `dataset`, `ground_truth`, `request_info`, `logprobs`\n",
        "\n",
        "**Note:** The saved `logprobs` are from **vLLM** at rollout time. To compare with local policy logprobs (e.g. for debugging `vllm_vs_local_logprob_diff_mean`), you would need to run a separate forward pass on the same tokens with the learner model; this notebook only inspects the stored vLLM logprobs and metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the rollouts directory (contains *_metadata.jsonl and *_rollouts_*.jsonl)\n",
        "# ROLLOUTS_DIR = \"/mnt/vast/home/lh22zyta/shortcut-RL/open-instruct/output/RLVR-soofi-basev2/rollouts\"\n",
        "ROLLOUTS_DIR = \"/mnt/vast/home/lh22zyta/shortcut-RL/open-instruct/output/RLVR-soofi-Olmo-IsomorphicRL/rollouts\"\n",
        "\n",
        "# Optional: restrict to a specific run (prefix of rollout filenames). If None, use first run found.\n",
        "# RUN_NAME = \"RLVR-soofi-basev2__1__1771322614\"  # or None for auto\n",
        "RUN_NAME = \"RLVR-soofi-Olmo-IsomorphicRL__1__1771443928\"  # or None for auto\n",
        "\n",
        "# Limit number of lines to read per file (None = no limit). Use for quick checks on huge files.\n",
        "MAX_LINES_PER_FILE = None  # e.g. 50_000\n",
        "\n",
        "# Step range to load (None = all steps). Reduces memory when inspecting a window.\n",
        "STEP_MIN = None  # e.g. 400\n",
        "STEP_MAX = None  # e.g. 600\n",
        "\n",
        "# How many full records to keep in memory for detailed inspection (rest are aggregated as stats only)\n",
        "NUM_SAMPLE_RECORDS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. List rollout files and load metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_rollout_files(rollouts_dir: str, run_name: str | None = None) -> tuple[list[str], str | None, dict | None]:\n",
        "    \"\"\"List rollout JSONL paths and return (paths, run_name, metadata_dict).\"\"\"\n",
        "    base = Path(rollouts_dir)\n",
        "    if not base.is_dir():\n",
        "        raise FileNotFoundError(f\"Not a directory: {rollouts_dir}\")\n",
        "\n",
        "    if run_name is None:\n",
        "        # Infer run_name from first metadata file\n",
        "        metas = sorted(base.glob(\"*_metadata.jsonl\"))\n",
        "        if not metas:\n",
        "            raise FileNotFoundError(f\"No *_metadata.jsonl in {rollouts_dir}\")\n",
        "        run_name = metas[0].stem.replace(\"_metadata\", \"\")\n",
        "\n",
        "    meta_path = base / f\"{run_name}_metadata.jsonl\"\n",
        "    metadata = None\n",
        "    if meta_path.exists():\n",
        "        with open(meta_path) as f:\n",
        "            metadata = json.loads(f.readline())\n",
        "\n",
        "    paths = sorted(base.glob(f\"{run_name}_rollouts_*.jsonl\"))\n",
        "    return [str(p) for p in paths], run_name, metadata\n",
        "\n",
        "\n",
        "paths, run_name, metadata = get_rollout_files(ROLLOUTS_DIR, RUN_NAME)\n",
        "print(f\"Run: {run_name}\")\n",
        "print(f\"Rollout files: {len(paths)}\")\n",
        "for p in paths:\n",
        "    size_mb = os.path.getsize(p) / (1024 * 1024)\n",
        "    print(f\"  {os.path.basename(p)}  ({size_mb:.1f} MB)\")\n",
        "if metadata:\n",
        "    print(f\"Metadata: {metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stream rollouts and aggregate stats (memory-efficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_rollouts(\n",
        "    paths: list[str],\n",
        "    step_min: int | None = None,\n",
        "    step_max: int | None = None,\n",
        "    max_lines_per_file: int | None = None,\n",
        "    num_sample_records: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterate over JSONL files line-by-line. For each record:\n",
        "    - Aggregate per-step stats (reward, advantage, response length, logprobs).\n",
        "    - Keep up to num_sample_records full records (spread across steps) for inspection.\n",
        "    \"\"\"\n",
        "    step_records = defaultdict(list)  # step -> list of lightweight dicts\n",
        "    sample_records = []  # full records for display\n",
        "    steps_seen = set()\n",
        "    total_lines = 0\n",
        "\n",
        "    for filepath in paths:\n",
        "        lines_read = 0\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                if max_lines_per_file is not None and lines_read >= max_lines_per_file:\n",
        "                    break\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    rec = json.loads(line)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Skip bad line in {filepath} line {lines_read + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                step = rec.get(\"step\", -1)\n",
        "                if step_min is not None and step < step_min:\n",
        "                    lines_read += 1\n",
        "                    total_lines += 1\n",
        "                    continue\n",
        "                if step_max is not None and step > step_max:\n",
        "                    lines_read += 1\n",
        "                    total_lines += 1\n",
        "                    continue\n",
        "\n",
        "                reward = rec.get(\"reward\", 0.0)\n",
        "                advantage = rec.get(\"advantage\", 0.0)\n",
        "                resp_tokens = rec.get(\"response_tokens\", [])\n",
        "                logprobs = rec.get(\"logprobs\")\n",
        "\n",
        "                stat = {\n",
        "                    \"reward\": reward,\n",
        "                    \"advantage\": advantage,\n",
        "                    \"response_len\": len(resp_tokens),\n",
        "                    \"prompt_len\": len(rec.get(\"prompt_tokens\", [])),\n",
        "                    \"finish_reason\": rec.get(\"finish_reason\", \"\"),\n",
        "                }\n",
        "                if logprobs is not None:\n",
        "                    valid = [x for x in logprobs if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "                    if valid:\n",
        "                        stat[\"logprob_mean\"] = float(np.mean(valid))\n",
        "                        stat[\"logprob_std\"] = float(np.std(valid))\n",
        "                        stat[\"logprob_min\"] = float(np.min(valid))\n",
        "                        stat[\"logprob_max\"] = float(np.max(valid))\n",
        "                    else:\n",
        "                        stat[\"logprob_mean\"] = stat[\"logprob_std\"] = stat[\"logprob_min\"] = stat[\"logprob_max\"] = None\n",
        "                else:\n",
        "                    stat[\"logprob_mean\"] = stat[\"logprob_std\"] = stat[\"logprob_min\"] = stat[\"logprob_max\"] = None\n",
        "\n",
        "                step_records[step].append(stat)\n",
        "\n",
        "                # Keep a few full records for inspection (one per step span)\n",
        "                if len(sample_records) < num_sample_records and step not in steps_seen:\n",
        "                    steps_seen.add(step)\n",
        "                    sample_records.append(rec)\n",
        "                elif len(sample_records) < num_sample_records and step in steps_seen:\n",
        "                    # Replace one of the samples with a later step to spread steps\n",
        "                    pass  # keep first occurrence per step\n",
        "\n",
        "                lines_read += 1\n",
        "                total_lines += 1\n",
        "\n",
        "    return dict(step_records), sample_records, total_lines\n",
        "\n",
        "\n",
        "step_records, sample_records, total_lines = stream_rollouts(\n",
        "    paths,\n",
        "    step_min=STEP_MIN,\n",
        "    step_max=STEP_MAX,\n",
        "    max_lines_per_file=MAX_LINES_PER_FILE,\n",
        "    num_sample_records=NUM_SAMPLE_RECORDS,\n",
        ")\n",
        "print(f\"Total records read: {total_lines}\")\n",
        "print(f\"Steps with data: {len(step_records)}\")\n",
        "if step_records:\n",
        "    steps = sorted(step_records.keys())\n",
        "    print(f\"Step range: {steps[0]} .. {steps[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = paths[0]\n",
        "file = open(p)\n",
        "j = json.loads(file.readline())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(j['logprobs'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-step summary (rewards, lengths, logprobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_step_summary(step_records: dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for step in sorted(step_records.keys()):\n",
        "        stats = step_records[step]\n",
        "        n = len(stats)\n",
        "        rewards = [s[\"reward\"] for s in stats]\n",
        "        advantages = [s[\"advantage\"] for s in stats]\n",
        "        resp_lens = [s[\"response_len\"] for s in stats]\n",
        "        prompt_lens = [s[\"prompt_len\"] for s in stats]\n",
        "        finish_stop = sum(1 for s in stats if s[\"finish_reason\"] == \"stop\")\n",
        "\n",
        "        row = {\n",
        "            \"step\": step,\n",
        "            \"n\": n,\n",
        "            \"reward_mean\": np.mean(rewards),\n",
        "            \"reward_std\": np.std(rewards),\n",
        "            \"reward_min\": np.min(rewards),\n",
        "            \"reward_max\": np.max(rewards),\n",
        "            \"advantage_mean\": np.mean(advantages),\n",
        "            \"resp_len_mean\": np.mean(resp_lens),\n",
        "            \"resp_len_max\": np.max(resp_lens),\n",
        "            \"prompt_len_mean\": np.mean(prompt_lens),\n",
        "            \"stop_rate\": finish_stop / n if n else 0,\n",
        "        }\n",
        "        logprob_means = [s[\"logprob_mean\"] for s in stats if s.get(\"logprob_mean\") is not None]\n",
        "        if logprob_means:\n",
        "            row[\"logprob_mean_mean\"] = np.mean(logprob_means)\n",
        "            row[\"logprob_mean_std\"] = np.std(logprob_means)\n",
        "        else:\n",
        "            row[\"logprob_mean_mean\"] = np.nan\n",
        "            row[\"logprob_mean_std\"] = np.nan\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "df = build_step_summary(step_records)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot reward and response length vs step (if matplotlib available)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(df[\"step\"], df[\"reward_mean\"], label=\"reward mean\")\n",
        "    ax.fill_between(df[\"step\"], df[\"reward_min\"], df[\"reward_max\"], alpha=0.2)\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"reward\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(df[\"step\"], df[\"resp_len_mean\"], label=\"response length mean\")\n",
        "    ax.plot(df[\"step\"], df[\"resp_len_max\"], alpha=0.7, label=\"response length max\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"tokens\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(df[\"step\"], df[\"stop_rate\"], color=\"green\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"stop_rate\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    if df[\"logprob_mean_mean\"].notna().any():\n",
        "        ax.plot(df[\"step\"], df[\"logprob_mean_mean\"], label=\"mean logprob (vLLM)\")\n",
        "    ax.set_xlabel(\"step\")\n",
        "    ax.set_ylabel(\"logprob\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"matplotlib not available; skip plots.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample full records (token IDs only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, rec in enumerate(sample_records):\n",
        "    print(f\"--- Sample {i + 1} (step={rec.get('step')}, sample_idx={rec.get('sample_idx')}) ---\")\n",
        "    print(f\"  reward={rec.get('reward')}, advantage={rec.get('advantage')}, finish_reason={rec.get('finish_reason')}\")\n",
        "    print(f\"  prompt_tokens: len={len(rec.get('prompt_tokens', []))}\")\n",
        "    print(f\"  response_tokens: len={len(rec.get('response_tokens', []))}\")\n",
        "    if rec.get(\"logprobs\"):\n",
        "        lp = rec[\"logprobs\"]\n",
        "        valid = [x for x in lp if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "        if valid:\n",
        "            print(f\"  logprobs: len={len(lp)}, mean={np.mean(valid):.4f}, min={np.min(valid):.4f}, max={np.max(valid):.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optional: decode tokens to text (requires tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer from metadata model_name (or set MODEL_NAME explicitly)\n",
        "MODEL_NAME = (metadata or {}).get(\"model_name\", \"allenai/OLMo-1B-7B\")  # fallback\n",
        "USE_TOKENIZER = True  # set False to skip tokenizer load and decode\n",
        "\n",
        "if USE_TOKENIZER:\n",
        "    try:\n",
        "        from transformers import AutoTokenizer\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "        print(f\"Loaded tokenizer: {MODEL_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load tokenizer: {e}\")\n",
        "        tokenizer = None\n",
        "else:\n",
        "    tokenizer = None\n",
        "    print(\"Skipping tokenizer (USE_TOKENIZER=False).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(sample_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if tokenizer is not None and sample_records:\n",
        "    for i, rec in enumerate(sample_records[:3]):  # decode first 3 only\n",
        "        print(f\"=== Sample {i + 1} (step={rec.get('step')}) ===\")\n",
        "        prompt_ids = rec.get(\"prompt_tokens\", [])\n",
        "        response_ids = rec.get(\"response_tokens\", [])\n",
        "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
        "        response_text = tokenizer.decode(response_ids, skip_special_tokens=False)\n",
        "        print(\"[Prompt] (first 500 chars)\")\n",
        "        print(prompt_text[:500])\n",
        "        print(\"[Response] (first 800 chars)\")\n",
        "        print(response_text[:800])\n",
        "        print()\n",
        "else:\n",
        "    print(\"No tokenizer or no sample records; skip decode.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inspect a single step in detail (re-run with STEP_MIN/STEP_MAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: stream again with a narrow step range and larger sample to get many records from one step\n",
        "def stream_one_step(paths: list[str], target_step: int, max_records: int = 64):\n",
        "    records = []\n",
        "    for filepath in paths:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                rec = json.loads(line)\n",
        "                if rec.get(\"step\") == target_step:\n",
        "                    records.append(rec)\n",
        "                    if len(records) >= max_records:\n",
        "                        return records\n",
        "    return records\n",
        "\n",
        "\n",
        "TARGET_STEP = 30  # change to step of interest (e.g. 400, 500)\n",
        "one_step_records = stream_one_step(paths, TARGET_STEP)\n",
        "print(f\"Records at step {TARGET_STEP}: {len(one_step_records)}\")\n",
        "if one_step_records:\n",
        "    r0 = one_step_records[0]\n",
        "    print(f\"  reward: {r0.get('reward')}, advantage: {r0.get('advantage')}, finish_reason: {r0.get('finish_reason')}\")\n",
        "    print(f\"  response_tokens length: {len(r0.get('response_tokens', []))}\")\n",
        "    if r0.get(\"logprobs\"):\n",
        "        lp = [x for x in r0[\"logprobs\"] if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "        if lp:\n",
        "            print(f\"  logprobs: mean={np.mean(lp):.4f}, std={np.std(lp):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare first and last rollouts (2 each)\n",
        "\n",
        "def _tail_lines(path: str, n: int):\n",
        "    # read last n non-empty lines (binary-safe)\n",
        "    with open(path, 'rb') as f:\n",
        "        f.seek(0, 2)\n",
        "        size = f.tell()\n",
        "        block = bytearray()\n",
        "        lines = []\n",
        "        pos = size\n",
        "        while pos > 0 and len(lines) <= n:\n",
        "            toread = min(4096, pos)\n",
        "            pos -= toread\n",
        "            f.seek(pos)\n",
        "            data = f.read(toread)\n",
        "            block = data + block\n",
        "            lines = block.splitlines()\n",
        "        return [l.decode('utf-8', 'ignore') for l in lines[-n:]]\n",
        "\n",
        "\n",
        "def get_first_last_rollouts(paths: list[str], n_each: int = 2):\n",
        "    first = []\n",
        "    last = []\n",
        "    # collect first records from files in order\n",
        "    for p in paths:\n",
        "        with open(p) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                first.append(json.loads(line))\n",
        "                if len(first) >= n_each:\n",
        "                    break\n",
        "        if len(first) >= n_each:\n",
        "            break\n",
        "    # collect last records from files in reverse order\n",
        "    for p in reversed(paths):\n",
        "        try:\n",
        "            tail = _tail_lines(p, n_each * 2)  # read a few to skip blanks\n",
        "        except Exception:\n",
        "            tail = []\n",
        "        for line in reversed(tail):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                last.append(json.loads(line))\n",
        "            except Exception:\n",
        "                continue\n",
        "            if len(last) >= n_each:\n",
        "                break\n",
        "        if len(last) >= n_each:\n",
        "            break\n",
        "    last = list(reversed(last))  # keep chronological order for the late set\n",
        "    return first, last\n",
        "\n",
        "\n",
        "def summarize_and_decode(records: list[dict], label: str, tokenizer=None, max_chars=800):\n",
        "    print(f'=== {label} ({len(records)} records) ===')\n",
        "    for i, r in enumerate(records):\n",
        "        print(f'-- {label} #{i+1}: step={r.get(\"step\")}, sample_idx={r.get(\"sample_idx\")}')\n",
        "        print(f'   reward={r.get(\"reward\")}, advantage={r.get(\"advantage\")}, finish_reason={r.get(\"finish_reason\")}')\n",
        "        prompt_ids = r.get('prompt_tokens', [])\n",
        "        resp_ids = r.get('response_tokens', [])\n",
        "        print(f'   prompt_len={len(prompt_ids)}, response_len={len(resp_ids)}')\n",
        "        lp = r.get('logprobs')\n",
        "        if lp:\n",
        "            valid = [x for x in lp if isinstance(x, (int, float)) and not (isinstance(x, float) and np.isnan(x))]\n",
        "            if valid:\n",
        "                print(f'   logprobs: n={len(lp)}, mean={np.mean(valid):.4f}, std={np.std(valid):.4f}')\n",
        "        if tokenizer is not None:\n",
        "            try:\n",
        "                ptext = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
        "                rtext = tokenizer.decode(resp_ids, skip_special_tokens=False)\n",
        "                print('   [Prompt] (first 300 chars)')\n",
        "                print(ptext[:].replace('\\n', ' '))\n",
        "                print('   [Response] (first 600 chars)')\n",
        "                print(rtext[:].replace('\\n', ' '))\n",
        "            except Exception as e:\n",
        "                print(f'   decode failed: {e}')\n",
        "        print()\n",
        "\n",
        "\n",
        "# Run comparison (2 earliest, 2 latest)\n",
        "early, late = get_first_last_rollouts(paths, n_each=2)\n",
        "# load tokenizer if available (reuse earlier `tokenizer` variable if set)\n",
        "tk = globals().get('tokenizer', None)\n",
        "summarize_and_decode(early, 'EARLY', tokenizer=tk)\n",
        "summarize_and_decode(late, 'LATE', tokenizer=tk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print one (or more) task-response examples per step (memory-efficient)\n",
        "\n",
        "def load_examples_by_step(paths: list[str], steps: list[int] | None = None, max_examples_per_step: int = 1):\n",
        "    \"\"\"Load up to `max_examples_per_step` examples per step from the rollouts.\"\"\"\n",
        "    examples = defaultdict(list)  # step -> list[rec]\n",
        "\n",
        "    for filepath in paths:\n",
        "        try:\n",
        "            with open(filepath) as f:\n",
        "                for raw in f:\n",
        "                    line = raw.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        rec = json.loads(line)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    step = rec.get('step')\n",
        "                    if step is None:\n",
        "                        continue\n",
        "                    if steps and step not in steps:\n",
        "                        continue\n",
        "                    if len(examples[step]) < max_examples_per_step:\n",
        "                        examples[step].append(rec)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return dict(examples)\n",
        "\n",
        "\n",
        "paths = sorted(paths)[-1:]\n",
        "\n",
        "examples = load_examples_by_step(paths, steps=None, max_examples_per_step=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_examples(examples: dict, tokenizer):\n",
        "    for step in sorted(examples.keys()):\n",
        "        # if step % 10 == 0:\n",
        "        for i, r in enumerate(examples[step]):\n",
        "            pids = r.get('prompt_tokens', [])\n",
        "            rids = r.get('response_tokens', [])\n",
        "            print(f'=== STEP {step}  ({i+1}/{len(examples[step])}) - dataset={r.get(\"dataset\")} ===')\n",
        "            print(f\"  reward={round(r.get('reward', 0), 2)}, advantage={round(r.get('advantage', 0), 2)}, finish_reason={r.get('finish_reason')},  prompt_len={len(pids)}, response_len={len(rids)}\")\n",
        "            try:\n",
        "                ptext = tokenizer.decode(pids, skip_special_tokens=False)\n",
        "                rtext = tokenizer.decode(rids, skip_special_tokens=False)\n",
        "                print('  [Prompt]')\n",
        "                print('   ', ptext[:].replace('\\n', ' '))\n",
        "                print('  [Response]')\n",
        "                print('   ', rtext[:].replace('\\n', ' '))\n",
        "            except Exception as e:\n",
        "                print('  decode failed:', e)\n",
        "steps = sorted(examples.keys())\n",
        "recent_steps = steps[:]\n",
        "# Run the per-step printer with default options (prints 1 example per step)\n",
        "print_examples(tokenizer=globals().get('tokenizer', None), examples={step: examples[step] for step in recent_steps})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples[328][0].keys()\n",
        "for k in ['step', 'sample_idx', 'prompt_idx', 'reward', 'advantage', 'finish_reason', 'dataset', 'ground_truth', 'request_info']:\n",
        "    print(f\"{k}: {examples[328][0].get(k)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples.keys()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "open-instruct",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
